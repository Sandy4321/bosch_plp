Notes
- data represents part measurements
- large number of anonymized features
- can group features based on production and station lines
        - L<prod line>_S<station line>_F<feature id>
- three main feature types: numerical, categorical, date
        - date: timestamp for when measurement was taken
        - n features
        - i_th date column corresponds to i-1_th feature where iE[1, n+1]
        - assuming sequential relation between measurements and dates
        - (e.g. x measurements => x dates => 1 column in train_date.csv)

Strategy
- analyze each feature set separately
- numerical patterns (i.e. direct pattern between numerical features)
        - is there an identifiable correlation between a numeric feature and classification tag
- unsure of categorical significance, need more research
- combine feature sets and identify relationship between types

Numerical Training Set
- need to identify relationships between features
- we can discard n-1 features if n features are numerically related (direct mapping)
- one of the easiest ways to view the relationship between features is to generate plots
        - brute force approach; last resort as sifting through plots will take long
        - problem: 970 x 970 grid of plots (will take a while to compute)
        - we can split it into chunks and view smaller sets at each time
        - can we split it intelligently or are we forced to do it iteratively for fixed chunk size
- can start by separating different types
        - only two distinct types: 'float64' and 'int64' where 'int64' has 2 counts
        - int64 counts: Id, Response (not useful)
- (Sep 18) might have to come back to numerical set => too large
- group features based on line and station
        - there seems to be only four lines (0, 1, 2, 3)
        - line 0: 24 stations, 168 features
        - line 1: 2 stations, 513 features
        - line 2: 3 stations, 42 features
        - line 3: 21 stations, 245 features
        - need to think about whether we run the prediction with the segment-specific analysis
        - alternative: combine all of the segments and perform analysis
- start with analysis of line 2 => 3 dataframes
        - smallest number of features
        - might give indication of how useful it is to segment the data based on line and station
        - dataframe identification df_0<station number>
- analysis of line 0 => 24 dataframes
        - look at all
        - dataframe identifcation df_0<station number>
- df_226
        - 15 features
        - nothing unusual in terms of value counts
        - NaN count conducted; want to check whether we can discard x parts from these features
        - we only discard a part from the station if total_measurements == 0
        - number of equal nan value counts = 13 => 13 features with the same number of nan counts
        - we want to determine which parts have len(df_226.columns) NaN's
        - use strictly_nan() function and pass transposed dataframe
                - takes a while to run as we're iterating over 1,183,747 columns and checkng each
        - we can then cut the dataframe to all non-nan parts and conduct feature analysis
        - 956,736 parts with strictly nan values
        - df_226_cut: newly created dataframe with significant parts
- df_226_cut
        - 15 features; 227,011 parts with data
        - run feature analysis (plots) and observe relations
        - generate 15 plots and try and keep scales consistent
                - 5 x 3 grid
                - use grid to compare data for each of the features
                - build generic plot building function
                - doesn't fare very well given the sheer size of the data
        - looking into more realistic feature engineering/selection algorithms

- Joan's suggestion
        - random sample the data set
        - ~10,000 records with all 970 features
        - worry about combining models once we've covered the entire sample set
        - POA
                - split dataset into segments (taking the first 300,000 records)
                - random sampling 10,000 records from the first 300,000

- 300,000 data set
        - split data set into failures and passes
        - want to see how features correspond to the part's response
                - plot feature value against response
                - might be able to determine thresholds for each feature (assuming binary tresholds)
        - 970 plots (will take time to run)
                - splitting plots into pieces
                - add params to function allowing specification of start and end indices for dataframe
        - decent output with each of the features plotted against the response
        - another added feature is switch frequency
                - number of times the response moves from 0 to 1 across all parts for a given feature
                - map feature values to 0 or 1 responses; remove all responses that have NaN values for the feature
                - check notebook for switch frequency plot
                - no frequency that exceeds 3000 => interest halt
        - conduct plot analysis of each feature against the switch frequency of said feature

Categorical Training Set
- 2.5 GB worth of data
- unable to load this data into a dataframe
